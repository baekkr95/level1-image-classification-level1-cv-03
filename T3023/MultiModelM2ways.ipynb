{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label(Head, Branch) Classifier\n",
    "- 하나의 Convolution 모델에서 3개의 FC Layer 브랜치를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model Template\n",
    "class Res50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        1. 위와 같이 생성자의 parameter 에 num_claases 를 포함해주세요.\n",
    "        2. 나만의 모델 아키텍쳐를 디자인 해봅니다.\n",
    "        3. 모델의 output_dimension 은 num_classes 로 설정해주세요.\n",
    "        \"\"\"\n",
    "        self.pretrain_model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
    "        self.pretrain_model.fc = torch.nn.Linear(in_features=2048, out_features=num_classes, bias=True) # resnet18.fc의 in_features의 크기는?\n",
    "        # torch.nn.init.xavier_uniform_(pretrain_model.fc.weight)\n",
    "        # stdv = 1.0/np.sqrt(512) # fully connected layer의 bias를 resnet18.fc in_feature의 크기의 1/root(n) 크기의 uniform 분산 값 중 하나로 설정해주세요! - Why? https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "        # pretrain_model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. 위에서 정의한 모델 아키텍쳐를 forward propagation 을 진행해주세요\n",
    "        2. 결과로 나온 output 을 return 해주세요\n",
    "        \"\"\"\n",
    "        x = self.pretrain_model.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Custom Model Template\n",
    "class MultiRes50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        1. 위와 같이 생성자의 parameter 에 num_claases 를 포함해주세요.\n",
    "        2. 나만의 모델 아키텍쳐를 디자인 해봅니다.\n",
    "        3. 모델의 output_dimension 은 num_classes 로 설정해주세요.\n",
    "        \"\"\"\n",
    "        self.res50 = Res50(num_classes)\n",
    "        self.res50.load_state_dict(torch.load(\"/opt/ml/workspace/code/model/res50CusDS3/best.pth\"))\n",
    "        self.res50.to(\"cpu\")\n",
    "        \n",
    "        self.res50 = nn.Sequential(*list(self.res50.pretrain_model.children())[:-1])\n",
    "\n",
    "        self.mask = nn.Linear(2048, 3, bias=True)\n",
    "        self.mask.load_state_dict(torch.load(\"/opt/ml/workspace/code/resnext50_32x4dfc3ways_maskv2.pt\"))\n",
    "\n",
    "        self.age = nn.Linear(2060, 3, bias=True)\n",
    "        self.gender = nn.Linear(2060, 3, bias=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. 위에서 정의한 모델 아키텍쳐를 forward propagation 을 진행해주세요\n",
    "        2. 결과로 나온 output 을 return 해주세요\n",
    "        \"\"\"\n",
    "        x = self.res50.forward(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        m = self.mask(x)\n",
    "\n",
    "        pred_mask = torch.argmax(m, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        base = torch.ones(x.shape[0],12)\n",
    "        if pred_mask == 0: \n",
    "            base = base * 0\n",
    "            x = torch.cat([x, base.to(device)], dim=1)\n",
    "        elif pred_mask == 1:\n",
    "            base = base * 10\n",
    "            x = torch.cat([x, base.to(device)], dim=1)\n",
    "        elif pred_mask == 2:\n",
    "            base = base * -10\n",
    "            x = torch.cat([x, base.to(device)], dim=1)\n",
    "        a = self.age(x)\n",
    "        g = self.gender(x)\n",
    "        \n",
    "        return {\"mask\":m, \"age\":a, \"gender\":g}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_freeze(model):\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            #print(param)\n",
    "            param.requires_grad = False\n",
    "            #print(param)\n",
    "        dfs_freeze(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_model = MultiRes50(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_freeze(mask_model.res50)\n",
    "dfs_freeze(mask_model.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(mask_model.to(device),input_size=(3,320,320))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 train data 1가지\n",
    "- mask, gender, age 각각 labeling을 한 데이터프레임 3가지\n",
    "- 총 4가지의 데이터프레임을 Dataset모듈에 넣을 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 기본 train 데이터 셋\n",
    "df = pd.read_csv('train_label.csv')\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    # # wear, incorrect, normal 3가지 클래스로 변경\n",
    "    # # 0~5, 6~11, 12~17끼리 묶는다\n",
    "    df_mask = df.copy()\n",
    "\n",
    "    def mask_label(x):\n",
    "        if x in [0,1,2,3,4,5]:\n",
    "            return 0    # wear\n",
    "        elif x in [6,7,8,9,10,11]:\n",
    "            return 1    # incorrect\n",
    "        else:\n",
    "            return 2    # not wear\n",
    "\n",
    "    df_mask['label'] = df_mask['label'].apply(mask_label)\n",
    "\n",
    "    # # 남성, 여성 2가지 클래스로 변경\n",
    "    # # [0,1,2,6,7,8,12,13,14], [3,4,5,9,10,11,15,16,17]\n",
    "    df_gender = df.copy()\n",
    "\n",
    "    def gender_label(x):\n",
    "        if x in [0,1,2,6,7,8,12,13,14]:\n",
    "            return 0    # male\n",
    "        else:\n",
    "            return 1    # female\n",
    "\n",
    "    df_gender['label'] = df_gender['label'].apply(gender_label)\n",
    "\n",
    "    # # 청년, 중년, 장년 3가지 클래스로 변경\n",
    "    # # [0,3,6,9,12,15], [1,4,7,10,13,16], [2,5,8,11,14,17]\n",
    "    df_age = df.copy()\n",
    "\n",
    "    def age_label(x):\n",
    "        if x in [0,3,6,9,12,15]:\n",
    "            return 0    # young\n",
    "        elif x in [1,4,7,10,13,16]:\n",
    "            return 1    # middle\n",
    "        else:\n",
    "            return 2    # old\n",
    "\n",
    "    df_age['label'] = df_age['label'].apply(age_label)\n",
    "\n",
    "    return df_mask['label'],df_age['label'],df_gender['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  정의\n",
    "- mask, gender, age 별로 label 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.df = df\n",
    "        self.image_data = self.df['path']   # x data, 이미지\n",
    "        self.image_label = self.df['label'] # y data, 레이블\n",
    "\n",
    "\n",
    "        self.mask_label, self.age_label, self.gender_label = split_data(df)\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.df['path'].iloc[idx]\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        dict_label = {\n",
    "            'class' : self.image_label.iloc[idx],\n",
    "            'mask' : self.mask_label.iloc[idx],\n",
    "            'gender' : self.gender_label.iloc[idx],\n",
    "            'age' : self.age_label.iloc[idx]\n",
    "        }\n",
    "\n",
    "        return img, dict_label\n",
    "\n",
    "class CusDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.image_data = self.df['path']   # x data, 이미지\n",
    "        self.image_label = self.df['label'] # y data, 레이블\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_data.iloc[idx])\n",
    "        label = self.image_label.iloc[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform & Dataset\n",
    "- 모든 데이터를 한방에 학습할 때와\n",
    "- train과 valid를 split할 때의 코드가 다름 (train, valid를 쪼갠거와 동일한 mask, gender, age 레이블링이 필요함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14175, 3) (4725, 3)\n"
     ]
    }
   ],
   "source": [
    "# Transform Compose\n",
    "data_transform = torchvision.transforms.Compose([\n",
    "    # transforms.CenterCrop(320),\n",
    "    torchvision.transforms.CenterCrop(320),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.2,0.2,0.2)),\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(df, test_size = 0.25, shuffle=True, stratify=df['label'], random_state=1234)\n",
    "print(train.shape, valid.shape)\n",
    "\n",
    "# train_dataset = CusDataset(train, data_transform)\n",
    "# test_dataset = CusDataset(valid, data_transform)\n",
    "\n",
    "# train data 한방에 학습시키는 경우\n",
    "train_dataset = MultiBranchDataset(train,data_transform)\n",
    "test_dataset = MultiBranchDataset(valid,data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- 여기서 3개로 나눌 필요가 없음\n",
    "- 모델 train에서 loss를 따로 나눌 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=50, \n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=50, \n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\" : train_dataloader,\n",
    "    \"test\" : test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터로더의 label 부분을 보면 mask, gender, age별로 배치사이즈에 맞게 레이블 정보가 담아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 1.6569,  1.6569,  1.6569,  ...,  1.5588,  1.5588,  1.5588],\n",
       "           [ 1.6569,  1.6569,  1.6569,  ...,  1.5588,  1.5588,  1.5588],\n",
       "           [ 1.6569,  1.6569,  1.6569,  ...,  1.5588,  1.5588,  1.5588],\n",
       "           ...,\n",
       "           [ 1.1863,  1.1863,  1.2059,  ..., -2.3627, -2.4412, -2.4608],\n",
       "           [ 1.0882,  1.0882,  1.1275,  ..., -2.3824, -2.4608, -2.4020],\n",
       "           [ 1.0490,  1.0490,  1.0686,  ..., -2.4412, -2.4804, -2.3824]],\n",
       " \n",
       "          [[ 1.6569,  1.6569,  1.6569,  ...,  1.5588,  1.5588,  1.5588],\n",
       "           [ 1.6569,  1.6569,  1.6569,  ...,  1.5588,  1.5588,  1.5588],\n",
       "           [ 1.6569,  1.6569,  1.6569,  ...,  1.5588,  1.5588,  1.5588],\n",
       "           ...,\n",
       "           [ 1.4216,  1.4216,  1.4216,  ..., -0.9510, -1.0490, -1.0686],\n",
       "           [ 1.4608,  1.4608,  1.4608,  ..., -0.9902, -1.0686, -1.0098],\n",
       "           [ 1.5000,  1.5000,  1.4804,  ..., -1.0490, -1.0882, -0.9902]],\n",
       " \n",
       "          [[ 1.6961,  1.6961,  1.6961,  ...,  1.5980,  1.5980,  1.5980],\n",
       "           [ 1.6961,  1.6961,  1.6961,  ...,  1.5980,  1.5980,  1.5980],\n",
       "           [ 1.6961,  1.6961,  1.6961,  ...,  1.5980,  1.5980,  1.5980],\n",
       "           ...,\n",
       "           [ 1.3824,  1.3824,  1.3431,  ..., -1.0294, -1.1275, -1.1471],\n",
       "           [ 1.3824,  1.3824,  1.3431,  ..., -1.0686, -1.1471, -1.0882],\n",
       "           [ 1.3824,  1.3824,  1.3431,  ..., -1.1275, -1.1667, -1.0686]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5000,  1.5000,  1.5000,  ..., -0.7353, -0.7549, -0.7549],\n",
       "           [ 1.5000,  1.5000,  1.5000,  ..., -0.7157, -0.7157, -0.7157],\n",
       "           [ 1.5000,  1.5000,  1.5000,  ..., -0.6765, -0.6765, -0.6765],\n",
       "           ...,\n",
       "           [ 1.2451,  1.2451,  1.2255,  ...,  1.0490,  1.0490,  1.0490],\n",
       "           [ 1.2451,  1.2451,  1.2255,  ...,  1.0490,  1.0490,  1.0490],\n",
       "           [ 1.2451,  1.2451,  1.2255,  ...,  1.0490,  1.0490,  1.0490]],\n",
       " \n",
       "          [[ 1.5392,  1.5392,  1.5392,  ...,  0.6373,  0.6176,  0.6176],\n",
       "           [ 1.5392,  1.5392,  1.5392,  ...,  0.6569,  0.6569,  0.6569],\n",
       "           [ 1.5392,  1.5392,  1.5392,  ...,  0.6961,  0.6961,  0.6961],\n",
       "           ...,\n",
       "           [ 1.1863,  1.1863,  1.1667,  ...,  1.0686,  1.0686,  1.0686],\n",
       "           [ 1.1863,  1.1863,  1.1667,  ...,  1.0686,  1.0686,  1.0686],\n",
       "           [ 1.1863,  1.1863,  1.1667,  ...,  1.0686,  1.0686,  1.0686]],\n",
       " \n",
       "          [[ 1.4412,  1.4412,  1.4412,  ...,  0.7941,  0.7745,  0.7745],\n",
       "           [ 1.4412,  1.4412,  1.4412,  ...,  0.8137,  0.8137,  0.8137],\n",
       "           [ 1.4412,  1.4412,  1.4412,  ...,  0.8529,  0.8529,  0.8529],\n",
       "           ...,\n",
       "           [ 1.0882,  1.0882,  1.0686,  ...,  0.7549,  0.7549,  0.7549],\n",
       "           [ 1.0882,  1.0882,  1.0686,  ...,  0.7549,  0.7549,  0.7549],\n",
       "           [ 1.0882,  1.0882,  1.0686,  ...,  0.7549,  0.7549,  0.7549]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0882,  1.0882,  1.1078,  ...,  0.6961,  0.6961,  0.6961],\n",
       "           [ 1.1078,  1.1078,  1.1275,  ...,  0.6961,  0.6961,  0.6961],\n",
       "           [ 1.1275,  1.1471,  1.1471,  ...,  0.6961,  0.6961,  0.6961],\n",
       "           ...,\n",
       "           [-2.2451, -2.2647, -2.2647,  ..., -2.3039, -2.3039, -2.3039],\n",
       "           [-2.2647, -2.3235, -2.2647,  ..., -2.2843, -2.2843, -2.2843],\n",
       "           [-2.2059, -2.2451, -2.3039,  ..., -2.2843, -2.2843, -2.2843]],\n",
       " \n",
       "          [[ 1.0882,  1.0882,  1.1078,  ...,  0.6373,  0.6373,  0.6373],\n",
       "           [ 1.1078,  1.1078,  1.1275,  ...,  0.6373,  0.6373,  0.6373],\n",
       "           [ 1.1275,  1.1471,  1.1471,  ...,  0.6373,  0.6373,  0.6373],\n",
       "           ...,\n",
       "           [-2.1275, -2.1471, -2.1471,  ..., -2.2647, -2.2647, -2.2647],\n",
       "           [-2.1471, -2.2059, -2.1471,  ..., -2.2451, -2.2451, -2.2451],\n",
       "           [-2.0882, -2.1275, -2.1863,  ..., -2.2451, -2.2451, -2.2451]],\n",
       " \n",
       "          [[ 0.8922,  0.8922,  0.9118,  ...,  0.5000,  0.5000,  0.5000],\n",
       "           [ 0.9118,  0.9118,  0.9314,  ...,  0.5000,  0.5000,  0.5000],\n",
       "           [ 0.9314,  0.9510,  0.9510,  ...,  0.5000,  0.5000,  0.5000],\n",
       "           ...,\n",
       "           [-2.1667, -2.1863, -2.1863,  ..., -2.3235, -2.3235, -2.3235],\n",
       "           [-2.1863, -2.2451, -2.1863,  ..., -2.3039, -2.3039, -2.3039],\n",
       "           [-2.1275, -2.1667, -2.2255,  ..., -2.3039, -2.3039, -2.3039]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.0294,  1.0294,  1.0294,  ...,  2.0294,  1.8725,  1.7549],\n",
       "           [ 1.0686,  1.0686,  1.0686,  ...,  1.3235,  1.2451,  1.2647],\n",
       "           [ 1.0686,  1.0686,  1.0490,  ...,  1.5392,  1.2451,  1.2255],\n",
       "           ...,\n",
       "           [-0.6373, -0.6961, -0.6961,  ...,  0.7745,  0.7549,  0.7549],\n",
       "           [-0.5980, -0.6373, -0.6373,  ...,  0.7549,  0.7353,  0.7353],\n",
       "           [-0.7745, -0.7549, -0.7157,  ...,  0.7353,  0.7353,  0.7353]],\n",
       " \n",
       "          [[-1.0098, -1.0098, -1.0098,  ...,  1.0882,  0.9510,  0.8922],\n",
       "           [-0.9510, -0.9510, -0.9510,  ..., -0.0294, -0.0882, -0.0294],\n",
       "           [-0.9510, -0.9510, -0.9706,  ..., -0.2843, -0.5196, -0.4804],\n",
       "           ...,\n",
       "           [-0.8922, -0.9510, -0.9510,  ...,  0.7745,  0.7549,  0.7549],\n",
       "           [-0.8529, -0.8922, -0.8922,  ...,  0.7549,  0.7353,  0.7353],\n",
       "           [-1.0294, -1.0098, -0.9706,  ...,  0.7353,  0.7353,  0.7353]],\n",
       " \n",
       "          [[-1.2647, -1.2647, -1.2647,  ...,  0.6569,  0.3627,  0.2059],\n",
       "           [-1.2451, -1.2451, -1.2451,  ..., -0.3235, -0.5000, -0.5392],\n",
       "           [-1.2843, -1.2843, -1.3039,  ..., -0.3824, -0.7353, -0.8137],\n",
       "           ...,\n",
       "           [-1.0098, -1.0686, -1.0686,  ...,  0.7353,  0.7157,  0.7157],\n",
       "           [-0.9706, -1.0098, -1.0098,  ...,  0.7157,  0.6961,  0.6961],\n",
       "           [-1.1471, -1.1275, -1.0882,  ...,  0.6961,  0.6961,  0.6961]]],\n",
       " \n",
       " \n",
       "         [[[ 1.4608,  1.4608,  1.4608,  ...,  0.7745,  0.7549,  0.7353],\n",
       "           [ 1.4608,  1.4608,  1.4608,  ...,  0.7745,  0.7353,  0.7353],\n",
       "           [ 1.4608,  1.4608,  1.4608,  ...,  0.7745,  0.7353,  0.7157],\n",
       "           ...,\n",
       "           [-2.0686, -1.6765, -0.7353,  ...,  0.6373,  0.6373,  0.6765],\n",
       "           [-1.7157, -1.6765, -1.3627,  ...,  0.6569,  0.6569,  0.6961],\n",
       "           [-1.8333, -1.8922, -1.7353,  ...,  0.6765,  0.6569,  0.7157]],\n",
       " \n",
       "          [[ 1.5588,  1.5588,  1.5588,  ..., -0.2059, -0.2255, -0.2451],\n",
       "           [ 1.5588,  1.5588,  1.5588,  ..., -0.2059, -0.2451, -0.2451],\n",
       "           [ 1.5588,  1.5588,  1.5588,  ..., -0.2059, -0.2451, -0.2647],\n",
       "           ...,\n",
       "           [-1.9706, -1.5784, -0.6373,  ..., -0.2647, -0.2647, -0.2255],\n",
       "           [-1.6176, -1.5784, -1.2647,  ..., -0.2451, -0.2451, -0.2059],\n",
       "           [-1.7353, -1.7941, -1.6373,  ..., -0.2255, -0.2451, -0.1863]],\n",
       " \n",
       "          [[ 1.4804,  1.4804,  1.4804,  ..., -1.3627, -1.3824, -1.4020],\n",
       "           [ 1.4804,  1.4804,  1.4804,  ..., -1.3627, -1.4020, -1.4020],\n",
       "           [ 1.4804,  1.4804,  1.4804,  ..., -1.3627, -1.4020, -1.4216],\n",
       "           ...,\n",
       "           [-1.8529, -1.4608, -0.5196,  ..., -1.4412, -1.4412, -1.4020],\n",
       "           [-1.5000, -1.4608, -1.1471,  ..., -1.4216, -1.4216, -1.3824],\n",
       "           [-1.6176, -1.6765, -1.5196,  ..., -1.4020, -1.4216, -1.3627]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3235,  1.3235,  1.3235,  ...,  1.2255,  1.2255,  1.2255],\n",
       "           [ 1.3235,  1.3235,  1.3235,  ...,  1.2255,  1.2255,  1.2255],\n",
       "           [ 1.3235,  1.3235,  1.3235,  ...,  1.2255,  1.2255,  1.2255],\n",
       "           ...,\n",
       "           [ 0.1667,  0.1471,  0.1078,  ...,  0.6765,  0.6765,  0.6765],\n",
       "           [ 0.1667,  0.1471,  0.1078,  ...,  0.6765,  0.6765,  0.6765],\n",
       "           [ 0.1667,  0.1471,  0.1078,  ...,  0.6765,  0.6765,  0.6765]],\n",
       " \n",
       "          [[ 1.5392,  1.5392,  1.5392,  ...,  1.4216,  1.4216,  1.4216],\n",
       "           [ 1.5392,  1.5392,  1.5392,  ...,  1.4216,  1.4216,  1.4216],\n",
       "           [ 1.5392,  1.5392,  1.5392,  ...,  1.4216,  1.4216,  1.4216],\n",
       "           ...,\n",
       "           [-0.6961, -0.7157, -0.7549,  ..., -0.0882, -0.0882, -0.0882],\n",
       "           [-0.6961, -0.7157, -0.7549,  ..., -0.0882, -0.0882, -0.0882],\n",
       "           [-0.6961, -0.7157, -0.7549,  ..., -0.0882, -0.0882, -0.0882]],\n",
       " \n",
       "          [[ 1.6569,  1.6569,  1.6569,  ...,  1.5980,  1.5980,  1.5980],\n",
       "           [ 1.6569,  1.6569,  1.6569,  ...,  1.5980,  1.5980,  1.5980],\n",
       "           [ 1.6569,  1.6569,  1.6569,  ...,  1.5980,  1.5980,  1.5980],\n",
       "           ...,\n",
       "           [-1.8922, -1.9118, -1.9902,  ..., -1.5980, -1.5980, -1.5980],\n",
       "           [-1.8922, -1.9118, -1.9902,  ..., -1.5980, -1.5980, -1.5980],\n",
       "           [-1.8922, -1.9118, -1.9902,  ..., -1.5980, -1.5980, -1.5980]]]]),\n",
       " {'class': tensor([ 4,  4,  0,  4,  1,  4,  0,  4,  0, 10,  7,  4,  6,  3, 16,  3,  0,  4,\n",
       "           5,  1,  0,  1,  0,  3,  7,  0, 10,  3,  9,  4,  4,  1, 12,  1,  3,  3,\n",
       "           7,  3,  1, 15,  0,  5, 13,  4, 16,  3,  0,  2,  4,  3]),\n",
       "  'mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'gender': tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "          0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "          1, 1]),\n",
       "  'age': tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0,\n",
       "          1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 1, 1, 0, 0, 2,\n",
       "          1, 0])}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv layer 빠져나오고 브랜치 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 함수 정의\n",
    "- 3가지 task 각각 loss 구하고 합침"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "leraning_book = [\"age\",\"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = mask_model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 3\n",
    "\n",
    "optimizer = {\n",
    "        \"mask\":torch.optim.Adam(mask_model.mask.parameters(), lr=LEARNING_RATE),\n",
    "        \"age\":torch.optim.Adam(mask_model.age.parameters(), lr=LEARNING_RATE),\n",
    "        \"gender\":torch.optim.Adam(mask_model.gender.parameters(), lr=LEARNING_RATE)\n",
    "    }\n",
    "#optimizer = torch.optim.Adam(mask_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "mask_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "age_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "gender_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = {\"mask\":mask_loss_fn,\"age\":age_loss_fn,\"gender\":gender_loss_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9a76f390354c268a86a89f9fa731f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.222, 평균 Accuracy : 0.953\n",
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.140, 평균 Accuracy : 0.977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c94cd0497464c999cc93a303193d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.069, 평균 Accuracy : 0.982\n",
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.029, 평균 Accuracy : 0.991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f78773701d94c9b98f6f8b8b935e6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.078, 평균 Accuracy : 0.980\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.028, 평균 Accuracy : 0.994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4204451adf6b4a568a3cacff121e154b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.055, 평균 Accuracy : 0.982\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.019, 평균 Accuracy : 0.991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e97cf04c6b4e8b890ccae1af6588eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.070, 평균 Accuracy : 0.981\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.022, 평균 Accuracy : 0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03dad5120d346e8ab6209ace6644601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.052, 평균 Accuracy : 0.982\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.016, 평균 Accuracy : 0.992\n",
      "학습 종료!\n",
      "최고 accuracy : 0.9822222590446472, 최고 낮은 loss : 0.05219957252185811\n",
      "학습 종료!\n",
      "최고 accuracy : 0.991534411907196, 최고 낮은 loss : 0.015796472365282987\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = {i:0.for i in leraning_book}\n",
    "best_test_loss = {i:9999.for i in leraning_book}\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = {i:0.for i in leraning_book}\n",
    "        running_acc = {i:0.for i in leraning_book}\n",
    "        if phase == \"train\":\n",
    "            target_model.train() # 네트워크 모델을 train 모드로 두어 gradient을 계산하고, 여러 sub module (배치 정규화, 드롭아웃 등)이 train mode로 작동할 수 있도록 함\n",
    "        elif phase == \"test\":\n",
    "            target_model.eval()\n",
    "\n",
    "        for ind, (images, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "            images = images.to(device)\n",
    "            labels = labels#.to(device) \n",
    "\n",
    "            for k in leraning_book:\n",
    "                optimizer[k].zero_grad()\n",
    "            loss = {}\n",
    "            preds = {}\n",
    "            with torch.set_grad_enabled(phase == \"train\"): # train 모드일 시에는 gradient를 계산하고, 아닐 때는 gradient를 계산하지 않아 연산량 최소화\n",
    "                logits = target_model(images)\n",
    "\n",
    "                for k,v in logits.items():\n",
    "                    if k in leraning_book:\n",
    "                        _, preds[k] = torch.max(v, 1)\n",
    "                        loss[k] = loss_fn[k](v, labels[k].to(device))\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    for k,v in loss.items():\n",
    "                        if k in leraning_book:\n",
    "                            loss[k].backward()\n",
    "                            optimizer[k].step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "            for k,v in loss.items():\n",
    "                if k in leraning_book:\n",
    "                    running_loss[k] += loss[k].item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "                    running_acc[k] += torch.sum(preds[k] == labels[k].data.to(device)) # 한 Batch에서의 Accuracy 값 저장\n",
    "\n",
    "        epoch_loss = {}\n",
    "        epoch_acc = {}\n",
    "        # 한 epoch이 모두 종료되었을 때,\n",
    "        for k in leraning_book:\n",
    "            epoch_loss[k] = running_loss[k] / len(dataloaders[phase].dataset)\n",
    "            epoch_acc[k] = running_acc[k] / len(dataloaders[phase].dataset)\n",
    "            print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss[k]:.3f}, 평균 Accuracy : {epoch_acc[k]:.3f}\")\n",
    "            if phase == \"test\" and best_test_accuracy[k] < epoch_acc[k]: # phase가 test일 때, best accuracy 계산\n",
    "                best_test_accuracy[k] = epoch_acc[k]\n",
    "            if phase == \"test\" and best_test_loss[k] > epoch_loss[k]: # phase가 test일 때, best loss 계산\n",
    "                best_test_loss[k] = epoch_loss[k]\n",
    "        # if i % 50 == 0:\n",
    "        #     print('Epoch: {}, i: {},Loss: {:.6f}'.format(epoch, i, running_loss))\n",
    "        #     print(f'{i}번 배치: {running_acc}/{(i+1)*64}, 정확도: {running_acc/((i+1)*64)}')\n",
    "for k in leraning_book: \n",
    "    print(\"학습 종료!\")\n",
    "    print(f\"최고 accuracy : {best_test_accuracy[k]}, 최고 낮은 loss : {best_test_loss[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트(Evaluation) 중, mask, gender, age별 output 뽑아내기\n",
    "- 최종 클래스 예측(18개)으로 변환\n",
    "- mask : 0, 1, 2 (wear, incorrect, not wear)\n",
    "- gender : 0, 1  (male, female)\n",
    "- age : 0, 1, 2  (young, middle, old)\n",
    "- class : (mask * 6) + (gender * 3) + (age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5f935ad4ce2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred_gender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_age\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "output = mask_model(image)\n",
    "pred_mask = torch.argmax(output['mask'], dim=-1)\n",
    "pred_gender = torch.argmax(output['gender'], dim=-1)\n",
    "pred_age = torch.argmax(output['age'], dim=-1)\n",
    "\n",
    "pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0673335b4e482bbfc4fee2a603393b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "dataset = TestDataset(image_paths, data_transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "test_model = target_model.to(device)\n",
    "test_model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        output = test_model(images)\n",
    "\n",
    "        pred_mask = torch.argmax(output['mask'], dim=-1)\n",
    "        pred_gender = torch.argmax(output['gender'], dim=-1)\n",
    "        pred_age = torch.argmax(output['age'], dim=-1)\n",
    "        pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)\n",
    "\n",
    "        all_predictions.extend(pred_class.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submissionM2ways.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_model.state_dict(), \"./resnext50_32x4dfc3waysv2.pt\")\n",
    "torch.save(target_model.mask.state_dict(), \"./resnext50_32x4dfc3ways_maskv2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train'\n",
    "valid = pd.read_csv(os.path.join(train_dir, 'train_label.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CusDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.image_data = self.df['path']   # x data, 이미지\n",
    "        self.image_label = self.df['label'] # y data, 레이블\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_data.iloc[idx])\n",
    "        label = self.image_label.iloc[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eval(raw_data, dataloader, model, device):\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i , (X,y) in enumerate(tqdm(dataloader)):\n",
    "            model_pred = model.forward(X.to(device))\n",
    "\n",
    "            pred_mask = torch.argmax(model_pred['mask'], dim=-1)\n",
    "            pred_gender = torch.argmax(model_pred['gender'], dim=-1)\n",
    "            pred_age = torch.argmax(model_pred['age'], dim=-1)\n",
    "            pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)\n",
    "\n",
    "            all_predictions.extend([[valid.iloc[i]['path'], pred_class.cpu().numpy()[0],y.cpu().numpy()[0]]])\n",
    "    #print(all_predictions)\n",
    "    result = pd.DataFrame(all_predictions, columns=['path', 'pred', 'target'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8aa0b5e552f4bcd94fb68cd7c5edff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  pred  target\n",
       "0      ../input/data/train/images/000001_female_Asian...     0       0\n",
       "1      ../input/data/train/images/000001_female_Asian...     0       0\n",
       "2      ../input/data/train/images/000001_female_Asian...     6       6\n",
       "3      ../input/data/train/images/000001_female_Asian...     4       4\n",
       "4      ../input/data/train/images/000001_female_Asian...    16      16\n",
       "...                                                  ...   ...     ...\n",
       "18895  ../input/data/train/images/006959_male_Asian_1...     0       0\n",
       "18896  ../input/data/train/images/006959_male_Asian_1...     3       3\n",
       "18897  ../input/data/train/images/006959_male_Asian_1...     9       9\n",
       "18898  ../input/data/train/images/006959_male_Asian_1...     7       7\n",
       "18899  ../input/data/train/images/006959_male_Asian_1...     3       3\n",
       "\n",
       "[18900 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = CusDataset(valid, data_transform)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, \n",
    "        num_workers=8,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "check_eval_df = check_eval(valid, valid_dataloader, target_model, device)\n",
    "check_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    }
   ],
   "source": [
    "wrong_df = check_eval_df[check_eval_df['pred'] != check_eval_df['target']]\n",
    "wrong_df = wrong_df.reset_index(drop=True)\n",
    "# wrong_df.head()\n",
    "print(len(wrong_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9652317371733111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(check_eval_df['target'], check_eval_df['pred'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  pred  target\n",
       "0      ../input/data/train/images/000001_female_Asian...     0       0\n",
       "1      ../input/data/train/images/000001_female_Asian...     2       0\n",
       "2      ../input/data/train/images/000001_female_Asian...     2       0\n",
       "3      ../input/data/train/images/000001_female_Asian...     0       0\n",
       "4      ../input/data/train/images/000001_female_Asian...     0       0\n",
       "...                                                  ...   ...     ...\n",
       "18895  ../input/data/train/images/006959_male_Asian_1...     0       0\n",
       "18896  ../input/data/train/images/006959_male_Asian_1...     0       0\n",
       "18897  ../input/data/train/images/006959_male_Asian_1...     0       0\n",
       "18898  ../input/data/train/images/006959_male_Asian_1...     0       0\n",
       "18899  ../input/data/train/images/006959_male_Asian_1...     0       0\n",
       "\n",
       "[18900 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = check_eval_df.copy()\n",
    "a = check_eval_df.copy()\n",
    "g = check_eval_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_label(x):\n",
    "    if x in [0,1,2,3,4,5]:\n",
    "        return 0    # wear\n",
    "    elif x in [6,7,8,9,10,11]:\n",
    "        return 1    # incorrect\n",
    "    else:\n",
    "        return 2    # not wear\n",
    "def gender_label(x):\n",
    "    if x in [0,1,2,6,7,8,12,13,14]:\n",
    "        return 0    # male\n",
    "    else:\n",
    "        return 1    # female\n",
    "def age_label(x):\n",
    "    if x in [0,3,6,9,12,15]:\n",
    "        return 0    # young\n",
    "    elif x in [1,4,7,10,13,16]:\n",
    "        return 1    # middle\n",
    "    else:\n",
    "        return 2    # old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['target'] = m['target'].apply(mask_label)\n",
    "m['pred'] = m['pred'].apply(mask_label)\n",
    "\n",
    "a['target'] = a['target'].apply(gender_label)\n",
    "a['pred'] = a['pred'].apply(gender_label)\n",
    "\n",
    "g['target'] = g['target'].apply(age_label)\n",
    "g['pred'] = g['pred'].apply(age_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf = m[m['pred'] != m['target']]\n",
    "adf = a[a['pred'] != a['target']]\n",
    "gdf = g[g['pred'] != g['target']]\n",
    "len(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990126473410653"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(m['target'], m['pred'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968760874589255"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(a['target'], a['pred'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97213729438098"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(g['target'], g['pred'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    8967\n",
       "1    8589\n",
       "2    1344\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.groupby(g.target).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0     17\n",
       "1     92\n",
       "2    129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.groupby([gdf.target]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 나이 모델 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = label_group.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = defaultdict(int)\n",
    "for k,v in temp.iterrows():\n",
    "    #print(k,v[0])\n",
    "    p[age_label(k)] += v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(p,columns=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 8967, 1: 8589, 2: 1344})"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#라벨 그룹별 수 구하기\n",
    "label_group = pd.DataFrame(df.groupby(\"label\").size().to_list())\n",
    "label_group.columns = ['count']\n",
    "label_group = label_group.reset_index().drop([\"index\"],axis=1)\n",
    "\n",
    "#평균보다 적은 라벨 리스트 구하기\n",
    "# mid_count = label_group.mean()\n",
    "# augmetaion_list = label_group[label_group < int(label_group.mean())].dropna()\n",
    "# augmetaion_list = augmetaion_list.sort_values(by=\"count\")\n",
    "\n",
    "#라벨명만 가져오기\n",
    "augmetaion_label = label_group.index.to_list()\n",
    "augmetaion_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_label(x):\n",
    "    if x in [0,3,6,9,12,15]:\n",
    "        return 0    # young\n",
    "    elif x in [1,4,7,10,13,16]:\n",
    "        return 1    # middle\n",
    "    elif x in [2,5,8,11,14,17]:\n",
    "        return 2    # old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_error = [2,5,8,11,14,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_df = df[df.label.isin(many_error)]\n",
    "path_add_df = []\n",
    "for i in many_error:\n",
    "    path_add_df.append(add_df[add_df.label == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = df.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmetaion_data = pd.concat([path_add_df[i] for i in range(len(many_error))] + [origin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "        #A.RandomRotate90(),\n",
    "        #A.Flip(),\n",
    "        #A.Transpose(),\n",
    "        A.OneOf([\n",
    "            #A.IAAAdditiveGaussianNoise(),\n",
    "            A.GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=25, p=0.4),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=.1),\n",
    "            #A.IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            #A.IAASharpen(),\n",
    "            #A.IAAEmboss(),\n",
    "            A.RandomBrightnessContrast(),            \n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.CenterCrop(320,320),\n",
    "        A.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.2, 0.2, 0.2],\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchDatasetAlbum(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.df = df\n",
    "        self.image_data = self.df['path']   # x data, 이미지\n",
    "        self.image_label = self.df['label'] # y data, 레이블\n",
    "\n",
    "\n",
    "        self.mask_label, self.age_label, self.gender_label = split_data(df)\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.df['path'].iloc[idx]\n",
    "        image = cv2.imread(self.image_data.iloc[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        \n",
    "        dict_label = {\n",
    "            'class' : self.image_label.iloc[idx],\n",
    "            'mask' : self.mask_label.iloc[idx],\n",
    "            'gender' : self.gender_label.iloc[idx],\n",
    "            'age' : self.age_label.iloc[idx]\n",
    "        }\n",
    "\n",
    "        return image, dict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmetaion_data_train = augmetaion_data.sample(frac=0.8)\n",
    "augmetaion_data_test = augmetaion_data.drop(augmetaion_data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiBranchDatasetAlbum(augmetaion_data_train, transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_dataset = MultiBranchDatasetAlbum(augmetaion_data_test, transform)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 64,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\" : train_loader,\n",
    "    \"test\" : test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_freeze(mask_model.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
