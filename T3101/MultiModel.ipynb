{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label(Head, Branch) Classifier\n",
    "- 하나의 Convolution 모델에서 3개의 FC Layer 브랜치를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 데이터셋 (train_df)\n",
    "- 기존 데이터셋에서 데이터 개수가 부족한 클래스들에 대해 albumentation 데이터를 추가 (aug_df)\n",
    "- train_df와 aug_df 를 합쳐서 데이터 불균형을 어느정도 해소하는 것이 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41289\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_label.csv')\n",
    "aug_df = pd.read_csv('augment.csv')\n",
    "aug_df2 = pd.read_csv('augment2.csv') # 2/27 추가\n",
    "aug_df3 = pd.read_csv('augment3.csv') # 2/27 추가\n",
    "\n",
    "concat_df = pd.concat([train_df,aug_df], ignore_index=True)\n",
    "concat_df2 = pd.concat([concat_df,aug_df2], ignore_index=True)\n",
    "\n",
    "\n",
    "df = pd.concat([concat_df2,aug_df3], ignore_index=True) # 최종 df\n",
    "\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# wear, incorrect, normal 3가지 클래스로 변경\n",
    "# 0~5, 6~11, 12~17끼리 묶는다\n",
    "df_mask = df.copy()\n",
    "\n",
    "def mask_label(x):\n",
    "    if x in [0,1,2,3,4,5]:\n",
    "        return 0    # wear\n",
    "    elif x in [6,7,8,9,10,11]:\n",
    "        return 1    # incorrect\n",
    "    else:\n",
    "        return 2    # not wear\n",
    "\n",
    "df_mask['label'] = df_mask['label'].apply(mask_label)\n",
    "\n",
    "# 남성, 여성 2가지 클래스로 변경\n",
    "# [0,1,2,6,7,8,12,13,14], [3,4,5,9,10,11,15,16,17]\n",
    "df_gender = df.copy()\n",
    "\n",
    "def gender_label(x):\n",
    "    if x in [0,1,2,6,7,8,12,13,14]:\n",
    "        return 0    # male\n",
    "    else:\n",
    "        return 1    # female\n",
    "\n",
    "df_gender['label'] = df_gender['label'].apply(gender_label)\n",
    "\n",
    "# 청년, 중년, 장년 3가지 클래스로 변경\n",
    "# [0,3,6,9,12,15], [1,4,7,10,13,16], [2,5,8,11,14,17]\n",
    "df_age = df.copy()\n",
    "\n",
    "def age_label(x):\n",
    "    if x in [0,3,6,9,12,15]:\n",
    "        return 0    # young\n",
    "    elif x in [1,4,7,10,13,16]:\n",
    "        return 1    # middle\n",
    "    else:\n",
    "        return 2    # old\n",
    "\n",
    "df_age['label'] = df_age['label'].apply(age_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAE9CAYAAAAMOst7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcy0lEQVR4nO3de7BlZX3m8e9jN4h3QI5IaEwzCTFBy6B2kMQkpRC5xQE0aMF4aQ0pcoFEM04SSGbiLaQ0E0PUiUyR0AKKIsFbD0OCLZqLqRFosEEuMraKoXuA7gjeYkkC/uaP/Xbcafp0n6bXOqfPy/dTteus9a613vfdZ3efZ693v3utVBWSJKlfj1roDkiSpHEZ9pIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUueWLnQHxrDffvvV8uXLF7obkiTNm+uvv/6fqmpmW9u6DPvly5ezdu3ahe6GJEnzJslXZ9vmML4kSZ0z7CVJ6pxhL0lS5wx7SZI6Z9hLktQ5w16SpM4Z9pIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUue6vDb+tM3nvX/Q+mZ+9ZWD1idJ0tg8s5ckqXOjh32SJUk+l+SKtn5wkmuSrE/yoSR7tvJHt/X1bfvyqTrObuW3Jzlm7D5LktST+Tizfx1w29T624Fzq+qHgfuA01r5acB9rfzcth9JDgVOAZ4BHAu8J8mSeei3JEldGDXskywDfh74i7Ye4Ejg8rbLRcBJbfnEtk7bflTb/0Tg0qq6v6q+AqwHDh+z35Ik9WTsM/s/BX4b+F5bfzLw9ap6oK1vAA5sywcCdwK07d9o+/9b+TaO+TdJTk+yNsnazZs3D/w0JElavEYL+yQvBjZV1fVjtTGtqs6vqhVVtWJmZmY+mpQkaVEY86t3zwdOSHI8sBfwROCdwN5Jlraz92XAxrb/RuAgYEOSpcCTgK9NlW8xfYwkSdqB0c7sq+rsqlpWVcuZTLD7VFW9Avg0cHLbbSXw8ba8uq3Ttn+qqqqVn9Jm6x8MHAJcO1a/JUnqzUJcVOd3gEuT/AHwOeCCVn4B8L4k64F7mbxBoKpuSXIZcCvwAHBGVT04/92WJGlxmpewr6q/Af6mLX+Zbcymr6rvAi+b5fhzgHPG66EkSf3yCnqSJHXOsJckqXOGvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1DnDXpKkzhn2kiR1zrCXJKlzhr0kSZ0z7CVJ6pxhL0lS5wx7SZI6Z9hLktQ5w16SpM4tXegO9ODu97xx0Pqe+mtvHrQ+SdIjm2f2kiR1zrCXJKlzhr0kSZ0z7CVJ6pxhL0lS50YL+yR7Jbk2yY1Jbkny5lZ+YZKvJFnXHoe18iR5V5L1SW5K8pypulYm+WJ7rByrz5Ik9WjMr97dDxxZVd9OsgfwmSR/1bb9VlVdvtX+xwGHtMfzgPOA5yXZF3gjsAIo4Pokq6vqvhH7LklSN0Y7s6+Jb7fVPdqjtnPIicDF7bjPAnsnOQA4BlhTVfe2gF8DHDtWvyVJ6s2on9knWZJkHbCJSWBf0zad04bqz03y6FZ2IHDn1OEbWtls5ZIkaQ5GDfuqerCqDgOWAYcneSZwNvCjwE8A+wK/M0RbSU5PsjbJ2s2bNw9RpSRJXZiX2fhV9XXg08CxVXVXG6q/H3gvcHjbbSNw0NRhy1rZbOVbt3F+Va2oqhUzMzMjPAtJkhanMWfjzyTZuy0/BngR8IX2OTxJApwE3NwOWQ28us3KPwL4RlXdBVwFHJ1knyT7AEe3MkmSNAdjzsY/ALgoyRImbyouq6orknwqyQwQYB3wK23/K4HjgfXAd4DXAlTVvUneClzX9ntLVd07Yr8lSerKaGFfVTcBz95G+ZGz7F/AGbNsWwWsGrSDkiQ9QngFPUmSOmfYS5LUOcNekqTOGfaSJHXOsJckqXOGvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1Lkxr42vAd103gmD1fWsX109WF2SpN2fZ/aSJHXOsJckqXOGvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1DnDXpKkzhn2kiR1zrCXJKlzhr0kSZ0z7CVJ6txoYZ9kryTXJrkxyS1J3tzKD05yTZL1ST6UZM9W/ui2vr5tXz5V19mt/PYkx4zVZ0mSejTmmf39wJFV9ePAYcCxSY4A3g6cW1U/DNwHnNb2Pw24r5Wf2/YjyaHAKcAzgGOB9yRZMmK/JUnqymhhXxPfbqt7tEcBRwKXt/KLgJPa8oltnbb9qCRp5ZdW1f1V9RVgPXD4WP2WJKk3o35mn2RJknXAJmAN8CXg61X1QNtlA3BgWz4QuBOgbf8G8OTp8m0cI0mSdmDUsK+qB6vqMGAZk7PxHx2rrSSnJ1mbZO3mzZvHakaSpEVnXmbjV9XXgU8DPwnsnWRp27QM2NiWNwIHAbTtTwK+Nl2+jWOm2zi/qlZU1YqZmZkxnoYkSYvSmLPxZ5Ls3ZYfA7wIuI1J6J/cdlsJfLwtr27rtO2fqqpq5ae02foHA4cA147Vb0mSerN0x7s8bAcAF7WZ848CLquqK5LcClya5A+AzwEXtP0vAN6XZD1wL5MZ+FTVLUkuA24FHgDOqKoHR+y3JEldGS3sq+om4NnbKP8y25hNX1XfBV42S13nAOcM3Ud931UXHD9ofcecduVDyt534bCXSHjVa64atD5J6pVX0JMkqXOGvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1DnDXpKkzhn2kiR1zrCXJKlzhr0kSZ0z7CVJ6tyYN8KR5t07Pjjs9fffcKrX35e0+HlmL0lS5wx7SZI6Z9hLktQ5w16SpM4Z9pIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUucMe0mSOmfYS5LUudHCPslBST6d5NYktyR5XSt/U5KNSda1x/FTx5ydZH2S25McM1V+bCtbn+SssfosSVKPxrwRzgPAG6rqhiRPAK5PsqZtO7eq/nh65ySHAqcAzwB+APhkkh9pm/8MeBGwAbguyeqqunXEvkuS1I3Rwr6q7gLuasvfSnIbcOB2DjkRuLSq7ge+kmQ9cHjbtr6qvgyQ5NK2r2EvSdIczMtn9kmWA88GrmlFZya5KcmqJPu0sgOBO6cO29DKZivfuo3Tk6xNsnbz5s1DPwVJkhat0cM+yeOBDwOvr6pvAucBPwQcxuTM/x1DtFNV51fViqpaMTMzM0SVkiR1YczP7EmyB5Ogv6SqPgJQVfdMbf9z4Iq2uhE4aOrwZa2M7ZRLkqQdGHM2foALgNuq6k+myg+Y2u0lwM1teTVwSpJHJzkYOAS4FrgOOCTJwUn2ZDKJb/VY/ZYkqTdjntk/H3gV8Pkk61rZ7wKnJjkMKOAO4JcBquqWJJcxmXj3AHBGVT0IkORM4CpgCbCqqm4Zsd+SJHVlzNn4nwGyjU1XbueYc4BztlF+5faOkyRJs/MKepIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUufmFPZJrp5LmSRJ2v1s96t3SfYCHgvs165hv+WrdE9k+ze1kSRJu4kdfc/+l4HXM7nl7PV8P+y/CfyP8bolSZKGst2wr6p3Au9M8utV9e556pMkSRrQnK6gV1XvTvJTwPLpY6rq4pH6JUmSBjKnsE/yPia3pV0HPNiKCzDsJUnazc312vgrgEOrqsbsjCRJGt5cv2d/M/DUMTsiSZLGMdcz+/2AW5NcC9y/pbCqThilV5IkaTBzDfs3jdkJaTF57UePHbS+977krx9SdvzH/ttg9V950lsHq0vS4jTX2fh/O3ZHJM2fn//Iewat73+/9NcGrU/SsOY6G/9bTGbfA+wJ7AH8c1U9cayOSZKkYcz1zP4JW5aTBDgROGKsTkmSpOHs9F3vauJjwDHDd0eSJA1trsP4L51afRST791/d5QeSZKkQc11Nv5/nFp+ALiDyVC+JEnazc31M/vXjt0RSZI0jrkO4y8D3g08vxX9PfC6qtqwnWMOYnLt/P2ZzOQ/v6remWRf4ENMbqpzB/DyqrqvTfx7J3A88B3gNVV1Q6trJfBfW9V/UFUX7cyTlDT/Xnz5JYPWd8XJr3hI2QmXXzFoG6tPfvGg9Um7i7kO478X+ADwsrb+ylb2ou0c8wDwhqq6IckTgOuTrAFeA1xdVW9LchZwFvA7wHHAIe3xPOA84HntzcEbmcwTqFbP6qq6b+5PU5Ienpd++LOD1veRX/CLTJp/c52NP1NV762qB9rjQmBmewdU1V1bzsyr6lvAbcCBTD7r33JmfhFwUls+Ebi4zfb/LLB3kgOYzPpfU1X3toBfAwx7CTNJkjo217D/WpJXJlnSHq8EvjbXRpIsB54NXAPsX1V3tU13Mxnmh8kbgTunDtvQymYrlyRJczDXsP9F4OVMwvku4GQmw/E7lOTxwIeB11fVN6e3tVvmDnLb3CSnJ1mbZO3mzZuHqFKSpC7MNezfAqysqpmqegqT8H/zjg5KsgeToL+kqj7Siu9pw/O0n5ta+UbgoKnDl7Wy2cr/nao6v6pWVNWKmZntfsIgSdIjylzD/lnTE+Kq6l4mw/KzarPrLwBuq6o/mdq0GljZllcCH58qf3UmjgC+0Yb7rwKOTrJPkn2Ao1uZJEmag7nOxn9Ukn22BH6bIb+jY58PvAr4fJJ1rex3gbcBlyU5Dfgqk48HAK5k8rW79Uy+evdamLyxSPJW4Lq231vamw1JkjQHcw37dwD/J8lftvWXAeds74Cq+gyQWTYftY39CzhjlrpWAavm2FdJkjRlrlfQuzjJWuDIVvTSqrp1vG5JkqShzPXMnhbuBrwkSYvMTt/iVpIkLS6GvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1DnDXpKkzhn2kiR1zrCXJKlzhr0kSZ0z7CVJ6pxhL0lS5wx7SZI6Z9hLktQ5w16SpM7N+X72kqRx/MZH7xysrne95KDB6lI/DHtJ6twHP7x50PpO/YWZh5T9w8XDtvH8Vz+0DT18DuNLktQ5w16SpM4Z9pIkdc7P7CVJi8Idf3r3oPUtf/1TH1J2z7k3DdrG/r/5rEHre7hGO7NPsirJpiQ3T5W9KcnGJOva4/ipbWcnWZ/k9iTHTJUf28rWJzlrrP5KktSrMYfxLwSO3Ub5uVV1WHtcCZDkUOAU4BntmPckWZJkCfBnwHHAocCpbV9JkjRHow3jV9XfJVk+x91PBC6tqvuBryRZDxzetq2vqi8DJLm07Xvr0P2VJKlXCzFB78wkN7Vh/n1a2YHA9FUlNrSy2colSdIczXfYnwf8EHAYcBfwjqEqTnJ6krVJ1m7ePOzFHSRJWszmNeyr6p6qerCqvgf8Od8fqt8ITF/jcVkrm618W3WfX1UrqmrFzIxXXpIkaYt5DfskB0ytvgTYMlN/NXBKkkcnORg4BLgWuA44JMnBSfZkMolv9Xz2WZKkxW60CXpJPgi8ANgvyQbgjcALkhwGFHAH8MsAVXVLksuYTLx7ADijqh5s9ZwJXAUsAVZV1S1j9VmSpB6NORv/1G0UX7Cd/c8BztlG+ZXAlQN2TZKkRxQvlytJUucMe0mSOmfYS5LUOcNekqTOGfaSJHXOsJckqXOGvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1DnDXpKkzhn2kiR1zrCXJKlzhr0kSZ0z7CVJ6pxhL0lS5wx7SZI6Z9hLktQ5w16SpM4Z9pIkdc6wlySpc4a9JEmdGy3sk6xKsinJzVNl+yZZk+SL7ec+rTxJ3pVkfZKbkjxn6piVbf8vJlk5Vn8lSerVmGf2FwLHblV2FnB1VR0CXN3WAY4DDmmP04HzYPLmAHgj8DzgcOCNW94gSJKkuRkt7Kvq74B7tyo+EbioLV8EnDRVfnFNfBbYO8kBwDHAmqq6t6ruA9bw0DcQkiRpO+b7M/v9q+qutnw3sH9bPhC4c2q/Da1stvKHSHJ6krVJ1m7evHnYXkuStIgt2AS9qiqgBqzv/KpaUVUrZmZmhqpWkqRFb77D/p42PE/7uamVbwQOmtpvWSubrVySJM3RfIf9amDLjPqVwMenyl/dZuUfAXyjDfdfBRydZJ82Me/oViZJkuZo6VgVJ/kg8AJgvyQbmMyqfxtwWZLTgK8CL2+7XwkcD6wHvgO8FqCq7k3yVuC6tt9bqmrrSX+SJGk7Rgv7qjp1lk1HbWPfAs6YpZ5VwKoBuyZJ0iOKV9CTJKlzhr0kSZ0z7CVJ6pxhL0lS5wx7SZI6Z9hLktS50b56J0mSHmrTuz85WF1P+fWfm9N+ntlLktQ5w16SpM4Z9pIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUucMe0mSOmfYS5LUOcNekqTOGfaSJHXOsJckqXOGvSRJnTPsJUnqnGEvSVLnFiTsk9yR5PNJ1iVZ28r2TbImyRfbz31aeZK8K8n6JDclec5C9FmSpMVqIc/sX1hVh1XVirZ+FnB1VR0CXN3WAY4DDmmP04Hz5r2nkiQtYrvTMP6JwEVt+SLgpKnyi2vis8DeSQ5YgP5JkrQoLVTYF/CJJNcnOb2V7V9Vd7Xlu4H92/KBwJ1Tx25oZZIkaQ6WLlC7P11VG5M8BViT5AvTG6uqktTOVNjeNJwO8LSnPW24nkqStMgtyJl9VW1sPzcBHwUOB+7ZMjzffm5qu28EDpo6fFkr27rO86tqRVWtmJmZGbP7kiQtKvMe9kkel+QJW5aBo4GbgdXAyrbbSuDjbXk18Oo2K/8I4BtTw/2SJGkHFmIYf3/go0m2tP+BqvrrJNcBlyU5Dfgq8PK2/5XA8cB64DvAa+e/y5IkLV7zHvZV9WXgx7dR/jXgqG2UF3DGPHRNkqQu7U5fvZMkSSMw7CVJ6pxhL0lS5wx7SZI6Z9hLktQ5w16SpM4Z9pIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUucMe0mSOmfYS5LUOcNekqTOGfaSJHXOsJckqXOGvSRJnTPsJUnqnGEvSVLnDHtJkjpn2EuS1DnDXpKkzhn2kiR1btGEfZJjk9yeZH2Ssxa6P5IkLRaLIuyTLAH+DDgOOBQ4NcmhC9srSZIWh0UR9sDhwPqq+nJV/QtwKXDiAvdJkqRFYbGE/YHAnVPrG1qZJEnagVTVQvdhh5KcDBxbVb/U1l8FPK+qzpza53Tg9Lb6dOD2nWxmP+CfBujuQrbRw3Owjd2nftvYvdro4TnYxrj1/2BVzWxrw9Jd78+82AgcNLW+rJX9m6o6Hzj/4TaQZG1VrXi4x+8ObfTwHGxj96nfNnavNnp4DraxcPUvlmH864BDkhycZE/gFGD1AvdJkqRFYVGc2VfVA0nOBK4ClgCrquqWBe6WJEmLwqIIe4CquhK4csQmHvZHALtRGz08B9vYfeq3jd2rjR6eg20sUP2LYoKeJEl6+BbLZ/aSJOlhMuyZXKEvyeeSXDFC3XsluTbJjUluSfLmgepdlWRTkpu3Kv/1JF9obf3R0G0keVOSjUnWtcfxu9LGVu29LsnNre+vH6rerdoY/bLLSfZOcnl7HW5L8pMD1Lmt1+K/tzZuSvLRJHvvajtbtXlHks+313ntQHVu63m8rL3m30uyy7OPZ2njQ1P/Zu9Ism5X29lee0NKclCSTye5tf2eXjdQvdv6Pb21/Xtal+QTSX5giLZa3U+feg3WJfnm0P/Pk/xm+x3dnOSDSfYaoM5ZX98kb0hSSfYbuo0k+yZZk+SL7ec+u9IGVfWIfwD/GfgAcMUIdQd4fFveA7gGOGKAen8WeA5w81TZC4FPAo9u608ZoY03Af9lhN/TM4GbgccymUvySeCHB25jCfAl4D8AewI3AoeO8FwuAn6pLe8J7D3S6300sLQtvx14+8DP4w5gv4Hr3Nbz+DEm18b4G2DFGG1stf0dwO+P+ZwG/p0dADynLT8B+L9D/Lud5bV44tTybwD/c6TntAS4m8n3woeq80DgK8Bj2vplwGvGen2ZfB38KuCru/r/ZJbX4o+As9ryWbv6//sRf2afZBnw88BfjFF/TXy7re7RHrs8UaKq/g64d6viXwXeVlX3t302jdDGWH4MuKaqvlNVDwB/C7x04DZGv+xykicx+Y97AUBV/UtVfX1X693Wa1FVn2i/K4DPMrn+xG5tludxW1Xt7EWwdqqNLZIEeDnwwflob6D676qqG9ryt4DbGOAKorO8Ft+cWn0cA/ytmsVRwJeq6qsD17sUeEySpUxOHP7frla4ndf3XOC3Ge/v+YlMThxoP0/alTYe8WEP/CmTF+x7YzXQPiZYB2wC1lTVNSM19SPAzyS5JsnfJvmJkdo5sw31rdrloaXvu5lJ35+c5LHA8fz7CykNYT4uu3wwsBl4b/to6C+SPG7gNrblF4G/GrjOAj6R5PpMrlDZg58B7qmqLy50Rx6OJMuBZzMZIRyrjXOS3Am8Avj9kZo5hQHfcAFU1Ubgj4F/BO4CvlFVnxiyjS2SnAhsrKobx6i/2b+q7mrLdwP770plj+iwT/JiYFNVXT9mO1X1YFUdxuTM6/AkzxypqaXAvsARwG8Bl7UzmSGdB/wQcBiT/1DvGKLSqrqNyVD0J4C/BtYBDw5R9zxbymQ47ryqejbwz0yG4EaT5PeAB4BLBq76p6vqOUzuNnlGkp8duP6FcCoDh8x8SfJ44MPA67c6Ax9UVf1eVR3E5N/TmTvaf2dlcmG0E4C/HLjefZicDR8M/ADwuCSvHLKN1s5jgd9lvDdCD1GTsfxdGkF4RIc98HzghCR3MBnSPTLJ+8dqrA3nfho4dqQmNgAfaR8dXMtktGKXJo5sraruaW9evgf8OZOh8aHqvqCqnltVPwvcx+SzySHt8LLLA9gAbJgavbmcSfiPIslrgBcDr2h/EAbTzpS2fBz0UQZ8rRdCG9p9KfChhe7LzkqyB5Ogv6SqPjJPzV4C/MII9R4H3FBV9wxc788BX6mqzVX1r8BHgJ8auA2YnOwcDNzYsmMZcEOSpw7czj1JDgBoP3fpY9lHdNhX1dlVtayqljMZVvpUVQ36TjDJzJZZ0kkeA7wI+MKQbUz5GJNJeiT5ESaTwwa9UcOWf3zNS5gMvw9V91Paz6cx+aP8gaHqbka/7HJV3Q3cmeTprego4NYh29giybFMPoI6oaq+M3Ddj0vyhC3LTCYDjjLbfB79HPCFqtqw0B3ZGW107gLgtqr6k5HbOmRq9UTG+Vs11ujKPwJHJHls+50dxWR+w6Cq6vNV9ZSqWt6yYwOTCZR3D9zUamBlW14JfHyXatuV2X09PYAXMM5s/GcBnwNuYvLHcpBZwEz+s9wF/CuTf2ynMQn397d2bgCOHKGN9wGfb89nNXDAgL+rv2cSjDcCR430Oh/PZMTgS8DvjdTGYcDa9jv6GLDPSK/3eiZzENa1x2Azp5l8Y+HG9rhlqN/VLM/jJW35fuAe4Kqh22jlFwK/MsLrvc32Bqz/p5kM4d409VofP9Jr8eH29+Mm4H8BBw78XB4HfA140tCvQ6v/zUzeoNzc/lY9euzXlwG+tTLLa/Fk4Grgi0y+nbTvrrThFfQkSercI3oYX5KkRwLDXpKkzhn2kiR1zrCXJKlzhr0kSZ0z7CXtUJJv72D78p2961uSC5OcvGs9kzQXhr0kSZ0z7CXNWZLHJ7k6yQ2Z3Ot++q6BS5NckuS2JJe3a4iT5LntxkzXJ7lqq6swSpoHhr2knfFd4CU1uUHOC4F3TN1s6enAe6rqx4BvAr/Wrun+buDkqnousAo4ZwH6LT2iLV3oDkhaVAL8YbsD3veY3CJ4y60376yqf2jL7wd+g8kdDJ8JrGnvCZYwuSyopHlk2EvaGa8AZoDnVtW/trt+7dW2bX3t7WLy5uCWqvrJ+euipK05jC9pZzwJ2NSC/oXAD05te1qSLaH+n4DPALcDM1vKk+yR5Bnz2mNJhr2knXIJsCLJ54FX8+9vgXo7cEaS24B9gPOq6l+Ak4G3J7mRyR3bxrjHuKTt8K53kiR1zjN7SZI6Z9hLktQ5w16SpM4Z9pIkdc6wlySpc4a9JEmdM+wlSeqcYS9JUuf+P/4lyAEn9pTOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=df, x='label', order = df['label'].value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  정의\n",
    "- mask, gender, age 별로 label 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchDataset(Dataset):\n",
    "    def __init__(self, df, df_mask, df_gender, df_age, transforms):\n",
    "        self.df = df\n",
    "        self.image_data = self.df['path']   # x data, 이미지\n",
    "        self.image_label = self.df['label'] # y data, 레이블\n",
    "\n",
    "        self.mask_label = df_mask['label']\n",
    "        self.gender_label = df_gender['label']\n",
    "        self.age_label = df_age['label']\n",
    "\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.df['path'].iloc[idx]\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        dict_label = {\n",
    "            'class' : self.image_label[idx],\n",
    "            'mask' : self.mask_label[idx],\n",
    "            'gender' : self.gender_label[idx],\n",
    "            'age' : self.age_label[idx]\n",
    "        }\n",
    "\n",
    "        return img, dict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14175, 3) (4725, 3)\n"
     ]
    }
   ],
   "source": [
    "# # 데이터셋 준비\n",
    "# df = pd.read_csv('train_label.csv')\n",
    "# df_mask = pd.read_csv('df_mask.csv')\n",
    "# df_gender = pd.read_csv('df_gender.csv')\n",
    "# df_age = pd.read_csv('df_age.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform & Dataset\n",
    "- 모든 데이터를 한방에 학습할 때와\n",
    "- train과 valid를 split할 때의 코드가 다름 (train, valid를 쪼갠거와 동일한 mask, gender, age 레이블링이 필요함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Compose\n",
    "data_transform = torchvision.transforms.Compose([\n",
    "    transforms.CenterCrop(350),\n",
    "    # torchvision.transforms.Resize((350,350),Image.BILINEAR),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.2,0.2,0.2)),\n",
    "])\n",
    "\n",
    "# train data 한방에 학습시키는 경우\n",
    "train_dataset = MultiBranchDataset(df, df_mask, df_gender, df_age, data_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26646, 3) (8883, 3)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 split할 경우, 기존 데이터(train, valid)와 이에 해당하는 mask, age, gender 레이블을 모두 맞춰야함\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(df, test_size = 0.25, shuffle=True, stratify=df['label'], random_state=1234)\n",
    "print(train.shape, valid.shape)\n",
    "\n",
    "train_ind = train.index.tolist()\n",
    "valid_ind = valid.index.tolist()\n",
    "\n",
    "train_df_mask = df_mask.loc[train_ind]\n",
    "valid_df_mask = df_mask.loc[valid_ind]\n",
    "\n",
    "train_df_gender = df_gender.loc[train_ind]\n",
    "valid_df_gender = df_gender.loc[valid_ind]\n",
    "\n",
    "train_df_age = df_age.loc[train_ind]\n",
    "valid_df_age = df_age.loc[valid_ind]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- 여기서 3개로 나눌 필요가 없음\n",
    "- 모델 train에서 loss를 따로 나눌 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터로더의 label 부분을 보면 mask, gender, age별로 배치사이즈에 맞게 레이블 정보가 담아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': tensor([ 5,  0, 16,  2, 10, 16,  1,  3, 11,  3,  6,  0,  3, 15, 14, 14,  8, 16,\n",
       "          6,  9,  8,  7,  2, 14, 17,  4, 10,  0, 16,  4, 16,  8,  1,  5,  4,  1,\n",
       "          1,  0,  5,  1,  9, 16, 12,  7, 17, 17, 13, 15, 12,  9,  3,  3,  5, 15,\n",
       "         13, 16,  1,  5,  5,  7, 15, 15,  4,  3, 10,  2,  3, 13, 15, 12,  1, 17,\n",
       "         12,  5,  3, 11, 16, 13,  4, 11, 15,  5, 15,  9,  2,  1, 10,  3,  8,  4,\n",
       "          3, 10, 15,  3, 14,  5, 15, 15, 16,  8,  3,  2,  5,  9,  6,  3,  9, 14,\n",
       "         14,  4, 16,  5, 15,  9, 15,  5, 15, 10,  7,  6, 17,  6, 10,  3, 16, 15,\n",
       "          9, 15]),\n",
       " 'mask': tensor([0, 0, 2, 0, 1, 2, 0, 0, 1, 0, 1, 0, 0, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 2,\n",
       "         2, 0, 1, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 2,\n",
       "         2, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 0, 2, 2, 2, 0, 2,\n",
       "         2, 0, 0, 1, 2, 2, 0, 1, 2, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 2, 0,\n",
       "         2, 2, 2, 1, 0, 0, 0, 1, 1, 0, 1, 2, 2, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 1,\n",
       "         2, 1, 1, 0, 2, 2, 1, 2]),\n",
       " 'gender': tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "         0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1]),\n",
       " 'age': tensor([2, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2, 1, 2, 2,\n",
       "         2, 1, 1, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 0,\n",
       "         0, 0, 0, 0, 2, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 1, 2,\n",
       "         0, 2, 0, 2, 1, 1, 1, 2, 0, 2, 0, 0, 2, 1, 1, 0, 2, 1, 0, 1, 0, 0, 2, 2,\n",
       "         0, 0, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 1, 1, 2, 0, 0, 0, 2, 0, 1, 1, 0,\n",
       "         2, 0, 1, 0, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cutmix 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 랜덤 위치에서 패치 만들지 않고, 가로 세로로 절반만 자르는 함수\n",
    "def half_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "\n",
    "    # 가로로 절반(왼쪽, 오른쪽), 세로로 절반(위쪽, 아랫쪽)\n",
    "    idx = random.randint(0,1)\n",
    "\n",
    "    bbx1 = [0, 0]\n",
    "    bby1 = [0, H//2]\n",
    "    \n",
    "    bbx2 = [W, W]\n",
    "    bby2 = [H//2, H]\n",
    "\n",
    "    return bbx1[idx], bby1[idx], bbx2[idx], bby2[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Label Classifier Model\n",
    "- Multi Sample Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "        # classifier 전까지 사용\n",
    "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        self.mask = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, 3, bias=True)\n",
    "        )\n",
    "        self.gender = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, 2, bias=True)\n",
    "        )\n",
    "        self.age = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, 3, bias=True)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # print('22: ', x.shape)\n",
    "\n",
    "        return {\n",
    "            'mask': self.mask(x),\n",
    "            'gender': self.gender(x),\n",
    "            'age': self.age(x)\n",
    "        }\n",
    "\n",
    "    # Loss 함수 구현 부분\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        mask_loss = F.cross_entropy(net_output['mask'], ground_truth['mask'].to(device))\n",
    "        gender_loss = F.cross_entropy(net_output['gender'], ground_truth['gender'].to(device))\n",
    "        age_loss = F.cross_entropy(net_output['age'], ground_truth['age'].to(device))\n",
    "\n",
    "        loss = mask_loss + gender_loss + age_loss\n",
    "\n",
    "        return loss #, {'mask' : mask_loss, 'gender' : gender_loss, 'age' : age_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiBranchModel()\n",
    "x = torch.ones((1,3,224,224))\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiBranchModel().to(device)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCH = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss Function은 Model Class에서 구현함\n",
    "\n",
    "# model = torch.nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moblienetv2\n",
    "# torchvision.models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 데이터 Train만 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, i: 0,Loss: 3.306540\n",
      "0번 배치: 3/64, 정확도: 0.046875\n",
      "Epoch: 0, i: 100,Loss: 0.271943\n",
      "100번 배치: 10959/6464, 정확도: 1.6953898668289185\n",
      "Epoch: 0, i: 200,Loss: 0.114967\n",
      "200번 배치: 22720/12864, 정확도: 1.7661690711975098\n",
      "Epoch: 0, i: 300,Loss: 0.121596\n",
      "300번 배치: 34808/19264, 정확도: 1.8068935871124268\n",
      "Final Accuracy : tensor(0.9101, device='cuda:0')\n",
      "Epoch: 1, i: 0,Loss: 0.072116\n",
      "0번 배치: 125/64, 정확도: 1.953125\n",
      "Epoch: 1, i: 100,Loss: 0.057322\n",
      "100번 배치: 12563/6464, 정확도: 1.9435334205627441\n",
      "Epoch: 1, i: 200,Loss: 0.214528\n",
      "200번 배치: 24997/12864, 정확도: 1.9431747198104858\n",
      "Epoch: 1, i: 300,Loss: 0.043553\n",
      "300번 배치: 37391/19264, 정확도: 1.940977931022644\n",
      "Final Accuracy : tensor(0.9699, device='cuda:0')\n",
      "Epoch: 2, i: 0,Loss: 0.046657\n",
      "0번 배치: 127/64, 정확도: 1.984375\n",
      "Epoch: 2, i: 100,Loss: 0.021976\n",
      "100번 배치: 12713/6464, 정확도: 1.9667388200759888\n",
      "Epoch: 2, i: 200,Loss: 0.018715\n",
      "200번 배치: 25320/12864, 정확도: 1.9682835340499878\n",
      "Epoch: 2, i: 300,Loss: 0.047603\n",
      "300번 배치: 37976/19264, 정확도: 1.9713454246520996\n",
      "Final Accuracy : tensor(0.9857, device='cuda:0')\n",
      "Epoch: 3, i: 0,Loss: 0.007535\n",
      "0번 배치: 128/64, 정확도: 2.0\n",
      "Epoch: 3, i: 100,Loss: 0.021913\n",
      "100번 배치: 12810/6464, 정확도: 1.9817450046539307\n",
      "Epoch: 3, i: 200,Loss: 0.043529\n",
      "200번 배치: 25434/12864, 정확도: 1.9771454334259033\n",
      "Epoch: 3, i: 300,Loss: 0.031208\n",
      "300번 배치: 38049/19264, 정확도: 1.9751349687576294\n",
      "Final Accuracy : tensor(0.9877, device='cuda:0')\n",
      "Epoch: 4, i: 0,Loss: 0.015592\n",
      "0번 배치: 128/64, 정확도: 2.0\n",
      "Epoch: 4, i: 100,Loss: 0.016797\n",
      "100번 배치: 12797/6464, 정확도: 1.9797338247299194\n",
      "Epoch: 4, i: 200,Loss: 0.014732\n",
      "200번 배치: 25475/12864, 정확도: 1.980332612991333\n",
      "Epoch: 4, i: 300,Loss: 0.009103\n",
      "300번 배치: 38192/19264, 정확도: 1.9825581312179565\n",
      "Final Accuracy : tensor(0.9910, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    running_acc = 0\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels # dict타입이라서 get_loss 함수 안에서 따로 cuda로 올림\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "\n",
    "\n",
    "        pred_mask = torch.argmax(output['mask'], dim=-1)\n",
    "        pred_gender = torch.argmax(output['gender'], dim=-1)\n",
    "        pred_age = torch.argmax(output['age'], dim=-1)\n",
    "        pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)\n",
    "\n",
    "        loss_train = model.get_loss(output, labels)\n",
    "        total_loss += loss_train.item()\n",
    "\n",
    "        loss_train.backward()       # gradient를 계산\n",
    "        optimizer.step()      # gradient descent\n",
    "    \n",
    "\n",
    "        running_acc += torch.sum(pred_class == labels['class'].to(device))\n",
    "\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {}, i: {},Loss: {:.6f}'.format(epoch, i, loss_train.item()))\n",
    "            print(f'{i}번 배치: {running_acc}/{(i+1)*batch_size}, 정확도: {running_acc/((i+1)*64)}')\n",
    "    \n",
    "    epoch_acc = running_acc / len(train_dataloader.dataset)\n",
    "    print('Final Accuracy :', epoch_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cutmix 추가한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, i: 0,Loss: 3.317724\n",
      "0번 배치: 6/128, 정확도: 0.046875\n",
      "Epoch: 0, i: 100,Loss: 1.708813\n",
      "100번 배치: 7498/12928, 정확도: 0.5799814462661743\n",
      "Epoch: 0, i: 200,Loss: 0.279735\n",
      "200번 배치: 15662/25728, 정확도: 0.6087530851364136\n",
      "Epoch: 0, i: 300,Loss: 1.504243\n",
      "300번 배치: 23803/38528, 정확도: 0.6178104281425476\n",
      "Final Accuracy : tensor(0.6231, device='cuda:0')\n",
      "Epoch: 1, i: 0,Loss: 1.546771\n",
      "0번 배치: 43/128, 정확도: 0.3359375\n",
      "Epoch: 1, i: 100,Loss: 1.486984\n",
      "100번 배치: 8013/12928, 정확도: 0.6198174357414246\n",
      "Epoch: 1, i: 200,Loss: 0.107381\n",
      "200번 배치: 16378/25728, 정확도: 0.6365826725959778\n",
      "Epoch: 1, i: 300,Loss: 1.466144\n",
      "300번 배치: 24623/38528, 정확도: 0.6390936374664307\n",
      "Final Accuracy : tensor(0.6404, device='cuda:0')\n",
      "Epoch: 2, i: 0,Loss: 0.146239\n",
      "0번 배치: 121/128, 정확도: 0.9453125\n",
      "Epoch: 2, i: 100,Loss: 1.627552\n",
      "100번 배치: 8618/12928, 정확도: 0.6666150689125061\n",
      "Epoch: 2, i: 200,Loss: 0.102581\n",
      "200번 배치: 16509/25728, 정확도: 0.6416743993759155\n",
      "Epoch: 2, i: 300,Loss: 1.465384\n",
      "300번 배치: 24914/38528, 정확도: 0.6466465592384338\n",
      "Final Accuracy : tensor(0.6421, device='cuda:0')\n",
      "Epoch: 3, i: 0,Loss: 0.025974\n",
      "0번 배치: 127/128, 정확도: 0.9921875\n",
      "Epoch: 3, i: 100,Loss: 0.043664\n",
      "100번 배치: 8467/12928, 정확도: 0.6549350023269653\n",
      "Epoch: 3, i: 200,Loss: 0.022472\n",
      "200번 배치: 16109/25728, 정확도: 0.6261271834373474\n",
      "Epoch: 3, i: 300,Loss: 1.469295\n",
      "300번 배치: 24728/38528, 정확도: 0.6418189406394958\n",
      "Final Accuracy : tensor(0.6471, device='cuda:0')\n",
      "Epoch: 4, i: 0,Loss: 1.490273\n",
      "0번 배치: 42/128, 정확도: 0.328125\n",
      "Epoch: 4, i: 100,Loss: 0.032714\n",
      "100번 배치: 8324/12928, 정확도: 0.6438737511634827\n",
      "Epoch: 4, i: 200,Loss: 0.010757\n",
      "200번 배치: 17644/25728, 정확도: 0.6857897639274597\n",
      "Epoch: 4, i: 300,Loss: 1.437591\n",
      "300번 배치: 26080/38528, 정확도: 0.6769102811813354\n",
      "Final Accuracy : tensor(0.6729, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    running_acc = 0\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels # dict타입이라서 get_loss 함수 안에서 따로 cuda로 올림\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ### cutmix\n",
    "        r = np.random.rand(1)\n",
    "        if r <= 0.5:\n",
    "            lam = 1/2\n",
    "            rand_index = torch.randperm(images.size()[0])\n",
    "            target_a = labels\n",
    "\n",
    "            # mask, gender, age 별로 labels[rand_index] 만들고 다시 합쳐서 get_loss에 넣어준다?\n",
    "            # target_b = labels[rand_index]\n",
    "            target_mask = labels['mask'][rand_index]\n",
    "            target_gender = labels['gender'][rand_index]\n",
    "            target_age = labels['age'][rand_index]\n",
    "\n",
    "            target_b = {'mask' : target_mask, 'gender' : target_gender, 'age' : target_age}\n",
    "            bbx1, bby1, bbx2, bby2 = half_bbox(images.size(), lam)\n",
    "            images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            pred_mask = torch.argmax(output['mask'], dim=-1)\n",
    "            pred_gender = torch.argmax(output['gender'], dim=-1)\n",
    "            pred_age = torch.argmax(output['age'], dim=-1)\n",
    "            pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)\n",
    "\n",
    "            loss_train = model.get_loss(output, target_a) * lam + model.get_loss(output, target_b) * (1. - lam)\n",
    "            total_loss += loss_train.item()\n",
    "\n",
    "            loss_train.backward()       # gradient를 계산\n",
    "            optimizer.step()      # gradient descent\n",
    "        \n",
    "        else:\n",
    "            output = model(images)\n",
    "\n",
    "            pred_mask = torch.argmax(output['mask'], dim=-1)\n",
    "            pred_gender = torch.argmax(output['gender'], dim=-1)\n",
    "            pred_age = torch.argmax(output['age'], dim=-1)\n",
    "            pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)\n",
    "\n",
    "            loss_train = model.get_loss(output, labels)\n",
    "            total_loss += loss_train.item()\n",
    "\n",
    "            loss_train.backward()       # gradient를 계산\n",
    "            optimizer.step()      # gradient descent\n",
    "\n",
    "\n",
    "        running_acc += torch.sum(pred_class == labels['class'].to(device))\n",
    "\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {}, i: {},Loss: {:.6f}'.format(epoch, i, loss_train.item()))\n",
    "            print(f'{i}번 배치: {running_acc}/{(i+1)*batch_size}, 정확도: {running_acc/((i+1)*batch_size)}')\n",
    "    \n",
    "    epoch_acc = running_acc / len(train_dataloader.dataset)\n",
    "    print('Final Accuracy :', epoch_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 테스트(Evaluation) 중, mask, gender, age별 output 뽑아내기\n",
    "- 최종 클래스 예측(18개)으로 변환\n",
    "- mask : 0, 1, 2 (wear, incorrect, not wear)\n",
    "- gender : 0, 1  (male, female)\n",
    "- age : 0, 1, 2  (young, middle, old)\n",
    "- class : (mask * 6) + (gender * 3) + (age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "dataset = TestDataset(image_paths, data_transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "test_model = model.to(device)\n",
    "test_model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        output = test_model(images)\n",
    "\n",
    "        pred_mask = torch.argmax(output['mask'], dim=-1)\n",
    "        pred_gender = torch.argmax(output['gender'], dim=-1)\n",
    "        pred_age = torch.argmax(output['age'], dim=-1)\n",
    "        pred_class = (pred_mask * 6) + (pred_gender * 3) + (pred_age)\n",
    "\n",
    "        all_predictions.extend(pred_class.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission11.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
