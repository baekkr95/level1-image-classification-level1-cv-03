{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StepLR 파라미터\n",
    "- step size마다 gamma 비율로 lr을 감소시킨다. (step_size 마다 gamma를 곱한다.)\n",
    "- optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
    "- step_size: 몇 epoch마다 lr을 감소시킬지가 step_size를 의미한다.\n",
    "- gamma: gamma 비율로 lr을 감소시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StepLR\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CosineAnnealingLR 파라미터\n",
    "- optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
    "- T_max: 최대 iteration 횟수\n",
    "- eta_min: 최소로 떨어질 수있는 learning rate default=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExponentialLR\n",
    "- learing rate decay가 exponential함수를 따른다.\n",
    "- optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
    "- gamma: lr을 감소시킬 때, 곱해지는 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
